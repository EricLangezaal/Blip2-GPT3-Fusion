{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLIP-2.3\n",
    "In this notebook we combine BLIP2 with GPT3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lcur1642/.conda/envs/dl2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
    "\n",
    "import openai\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model and processor\n",
    "We can instantiate the model and its corresponding processor from the [hub](https://huggingface.co/models?other=blip-2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/lcur1642/.conda/envs/dl2/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda113.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/lcur1642/.conda/envs/dl2/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /home/lcur1642/.conda/envs/dl2/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lcur1642/.conda/envs/dl2/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/home/lcur1642/.conda/envs/dl2/lib/libcudart.so'), PosixPath('/home/lcur1642/.conda/envs/dl2/lib/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:51<00:00, 55.99s/it]\n"
     ]
    }
   ],
   "source": [
    "name = \"Salesforce/blip2-opt-2.7b\"\n",
    "processor = AutoProcessor.from_pretrained(name)\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(name, device_map=\"auto\", load_in_8bit=True) # load in int8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup GPT3 API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = \"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\"\n",
    "openai.api_key = openai_api_key"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://storage.googleapis.com/sfr-vision-language-research/LAVIS/assets/merlion.png' \n",
    "image = Image.open(requests.get(url, stream=True).raw).convert('RGB')   \n",
    "# display(image.resize((596, 437)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BLIP2_image_captioning(image, model, processor, device, prompt=None):\n",
    "    \"\"\"\"\n",
    "    Takes as input an image, model, processor, device and (optionally) a prompt and\n",
    "    returns a (prompted) caption for this image, created by BLIP2.\n",
    "    \"\"\"\n",
    "\n",
    "    if prompt:\n",
    "        inputs = processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n",
    "    else:\n",
    "        inputs = processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=20)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "\n",
    "def prompt_instruct_gpt(prompt, max_tokens=64, temperature=0, stop=None):\n",
    "  \"\"\"\n",
    "  Helper function for prompting the GPT3 instruct language model\n",
    "  \"\"\"\n",
    "\n",
    "  response = openai.Completion.create(engine=\"text-davinci-002\", prompt=prompt, max_tokens=max_tokens, temperature=temperature, stop=stop)\n",
    "  return response[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "def GPT3_image_captioning(image, model, processor,device, prompt=None):\n",
    "    \"\"\"\"\n",
    "    Takes as input an image, model, processor, device and (optionally) a prompt and\n",
    "    returns a (prompted) caption for this image, improved by GPT3.\n",
    "    \"\"\"\n",
    "    generated_text = BLIP2_image_captioning(image, model, processor, device, prompt)\n",
    "\n",
    "    prompt_GPT = f\"Instruction: augment or improve the answer. If the given answer is factually wrong, correct it \\\n",
    "    in a similar answer style. \\n Context: {prompt}. {generated_text}\"\n",
    "\n",
    "    return prompt_instruct_gpt(prompt_GPT)\n",
    "\n",
    "\n",
    "def prompt_chat_gpt(prompt, max_tokens=64, temperature=0.4, stop=None):\n",
    "  \"\"\"\n",
    "  Helper function for prompting the GPT3 chat-based language model\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an intelligent chatbot and rewrite answers of another question answering chatbot in a similar style\"},\n",
    "        {\"role\": \"user\", \"content\": (\"User question: 'Can I wear this for my trip to Canada in December?' \\\n",
    "                chatbot answer: yes, it's a nice shirt and shorts, but it's a little too casual for a trip to Canada. \")},\n",
    "        {\"role\": \"assistant\", \"content\": \"No, a shirt and shorts are not suitable for the freezing temperatures in Canada during December.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "    max_tokens = 64)\n",
    "\n",
    "  return response[\"choices\"][0]['message'][\"content\"].strip()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def chatGPT3_image_captioning(image, model, processor,device, prompt=None, example_input = None):\n",
    "    \"\"\"\"\n",
    "    Takes as input an image, model, processor, device and (optionally) a prompt and\n",
    "    returns a (prompted) caption for this image, improved by GPT3.\n",
    "    \"\"\"\n",
    "\n",
    "    if prompt:\n",
    "      print(prompt)\n",
    "      generated_text = BLIP2_image_captioning(image, model, processor, device, prompt)\n",
    "      prompt_GPT = f\"User question: {prompt}. Chatbot answer: {generated_text}\"\n",
    "    elif example_input:\n",
    "      print(example_input)\n",
    "      prompt_GPT = example_input  \n",
    "\n",
    "    return prompt_chat_gpt(prompt_GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User question: 'Write a famous quote said by this person'                 chatbot answer: albert einstein - the world is a book, and those who do not travel read                 only one page. \n"
     ]
    }
   ],
   "source": [
    "prompt = \"Question: Write a famous qiote said by this person Answer:\"\n",
    "example_input = (\"User question: 'Write a famous quote said by this person' \\\n",
    "                chatbot answer: albert einstein - the world is a book, and those who do not travel read \\\n",
    "                only one page. \")\n",
    "#caption_BLIP = BLIP2_image_captioning(image, model, processor, device, prompt)\n",
    "#caption_GPT3 = GPT3_image_captioning(image, model, processor, device, prompt)\n",
    "\n",
    "caption_chatGPT3 = chatGPT3_image_captioning(image, model, processor, device, example_input = example_input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Singapore.\n"
     ]
    }
   ],
   "source": [
    "print(caption_GPT3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "singapore\n"
     ]
    }
   ],
   "source": [
    "print(caption_BLIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Albert Einstein once said, \"Logic will get you from A to B. Imagination will take you everywhere.\"\n"
     ]
    }
   ],
   "source": [
    "print(caption_chatGPT3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL2kernel",
   "language": "python",
   "name": "dl2kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
